"""
This modified version only works with the rd64-uni-refined.pth weights 
wget https://owncloud.gwdg.de/index.php/s/ioHbRzFx6th32hn/download -O weights.zip
"""
import math
import torch
from torch import nn
from torch.nn import functional as nnf
from torchvision import transforms
from copy import deepcopy
from collections import OrderedDict

def get_conv1_size(img_size, stride):
    return int((img_size - 1) / stride + 1)

# Classes from OpenAI CLIP
class QuickGELU(nn.Module):
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)

class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = nn.LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype = x.dtype, device = x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights = False, attn_mask = self.attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

class Transformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])

    def forward(self, x: torch.Tensor):
        return self.resblocks(x)

# Generates the activations needed for CLIPSeg decoder
# rescaled_pos_emb and class_embedding_reshaped don't exist in CLIP, 
# therefore it's needed to load the weights generated by clipseg_mod_gen.CLIPActivationsGen
# and the generated values WILL CHANGE if the image size is changed.
class CLIPActivationsBase(nn.Module):
    def __init__(self):
        super().__init__()
    
    def rescale_pos_emb(self, img_size, stride = 16, token_shape = (14, 14)):
        new_size = get_conv1_size(img_size, stride)
        a = self.positional_embedding[1:].T.view(1, self.width, *token_shape)
        b = nnf.interpolate(a, (new_size, new_size), 
                                mode = 'bicubic', align_corners = False).squeeze(0).view(self.width, new_size * new_size).T
        return torch.cat([self.positional_embedding[:1], b])[None, :, :]

    def forward_multihead_attention(self, x, b):
        """ 
        Simplified version of multihead attention (taken from torch source code but without tons of if clauses). 
        The mlp and layer norm come from CLIP.
        x: input.
        b: multihead attention module. 
        """
        x_ = b.ln_1(x)
        q, k, v = nnf.linear(x_, b.attn.in_proj_weight, b.attn.in_proj_bias).chunk(3, dim = -1)
        tgt_len, bsz, embed_dim = q.size()

        q = q.contiguous().view(tgt_len, bsz * b.attn.num_heads, b.attn.head_dim).transpose(0, 1)
        k = k.contiguous().view(-1, bsz * b.attn.num_heads, b.attn.head_dim).transpose(0, 1)
        v = v.contiguous().view(-1, bsz * b.attn.num_heads, b.attn.head_dim).transpose(0, 1)

        q = q * self.scaling

        attn_output_weights = torch.bmm(q, k.transpose(1, 2)) #  n_heads * batch_size, tokens^2, tokens^2
        
        attn_output_weights = torch.softmax(attn_output_weights, dim = -1)

        attn_output = torch.bmm(attn_output_weights, v)
        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn_output = b.attn.out_proj(attn_output)

        x = x + attn_output
        x = x + b.mlp(b.ln_2(x))

        return x

    def visual_forward(self, x_inp):
        with torch.no_grad():
            x = self.conv1(x_inp)  # shape = [*, width, grid, grid]

            x = x.reshape(1, self.width, -1)  # shape = [*, width, grid ** 2]
            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]

            # Since this version is supposed to always use batch size one and images with a fixed size, 
            # Tensor filled with zeros will be always the same and it could be recycled
            x = torch.cat([self.class_embedding_reshaped, x], dim = 1)  # shape = [*, grid ** 2 + 1, width]

            # When using the rd64-uni-refined.pth, 
            # the optimal input size is 352 x 352
            x = x + self.rescaled_pos_emb
            
            # If the input image is 224 x 224 (original CLIP), it will use the 
            # original positional embeddings from CLIP without any interpolation
            x = self.ln_pre(x)

            x = x.permute(1, 0, 2)  # NLD -> LND

            activations = [] # Extract_layers 3, 6, 9
            
            res_block = self.transformer.resblocks[0]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[1]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[2]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[3]
            x = self.forward_multihead_attention(x, res_block)
            activations += [x]

            res_block = self.transformer.resblocks[4]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[5]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[6]
            x = self.forward_multihead_attention(x, res_block)
            activations += [x]

            res_block = self.transformer.resblocks[7]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[8]
            x = self.forward_multihead_attention(x, res_block)

            res_block = self.transformer.resblocks[9]
            x = self.forward_multihead_attention(x, res_block)
            activations += [x] # CLIPSeg only needs the attentions up to this point

            # Is it better to return a tuple or the list?
            return activations[::-1]
    
    def forward(self, inp_image):
        inp_image = inp_image.to(self.rescaled_pos_emb.device)
        activations = self.visual_forward(inp_image)

        return activations

class CLIPActivations(CLIPActivationsBase):
    def __init__(self, img_size = 352):
        super().__init__()
        clip_input_size = 224
        self.width = 768
        patch_size = 16
        layers = 10 # CLIP used here has 12 layers, but we only need 10
        heads = 12

        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = self.width, kernel_size = patch_size, stride = patch_size, bias = False)

        self.class_embedding_reshaped = nn.Parameter(torch.zeros(1, 1, self.width))
        self.positional_embedding = nn.Parameter(torch.zeros((clip_input_size // patch_size) ** 2 + 1, self.width))
        self.ln_pre = nn.LayerNorm(self.width)
        self.transformer = Transformer(self.width, layers, heads)

        self.rescaled_pos_emb = nn.Parameter(self.rescale_pos_emb(img_size, 
                                                                  stride = patch_size, 
                                                                  token_shape = (clip_input_size // patch_size, clip_input_size // patch_size)))

        head_dim = self.width // heads
        self.scaling = float(head_dim) ** -.5

        for p in self.parameters():
            p.requires_grad_(False) # Whole model is non-trainable now

class CLIPSegDecoderProcessConditional(nn.Module):
    def __init__(self):
        super().__init__()
        embed_dim = 512
        reduce_dim = 64
        self.film_mul = nn.Linear(embed_dim, reduce_dim)
        self.film_add = nn.Linear(embed_dim, reduce_dim)

        for p in self.parameters():
            p.requires_grad_(False) # Whole model is non-trainable now
            
    def forward(self, conditional):
        # When the prompts are reused, this can be recycled too
        return self.film_mul(conditional), self.film_add(conditional)

class CLIPSegDecoder(nn.Module):
    def __init__(self, img_size = 352, batch_size = 1):
        super().__init__()

        width = 768
        patch_size = 16
        self.reduce_dim = 64
        n_heads = 4
        depth = 3  # It uses the layers 3, 6 and 9 extracted from CLIP
        self.batch_size = batch_size # This way, the model won't have a variable output shape
        
        trans_conv_ks = (patch_size, patch_size)

        tp_kernels = (trans_conv_ks[0] // 4, trans_conv_ks[0] // 4)

        self.trans_conv = nn.Sequential(
            nn.Conv2d(self.reduce_dim, self.reduce_dim, kernel_size = 3, padding = 1),
            nn.ReLU(),
            nn.ConvTranspose2d(self.reduce_dim, self.reduce_dim // 2, kernel_size = tp_kernels[0], stride = tp_kernels[0]),
            nn.ReLU(),
            nn.ConvTranspose2d(self.reduce_dim // 2, 1, kernel_size = tp_kernels[1], stride=tp_kernels[1]),               
        )

        self.reduces = nn.ModuleList([nn.Linear(width, self.reduce_dim) for _ in range(depth)])
        self.blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model = self.reduce_dim, nhead = n_heads) for _ in range(depth)])

        self.size = int(math.sqrt(get_conv1_size(img_size, stride = patch_size) ** 2 + 1))

        for p in self.parameters():
            p.requires_grad_(False) # Whole model is non-trainable now

    def forward(self, conditional_mul, conditional_add, activations_3, activations_6, activations_9):
        a = self.reduces[0](activations_3)
        a = conditional_mul * a + conditional_add
        a = self.blocks[0](a)

        a = self.reduces[1](activations_6) + a
        a = self.blocks[1](a)

        a = self.reduces[2](activations_9) + a
        a = self.blocks[2](a)

        a = a[1:].permute(1, 2, 0) # rm cls token and -> BS, Feats, Tokens

        a = a.reshape(self.batch_size, self.reduce_dim, self.size, self.size)

        a = self.trans_conv(a)

        return a
